{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80390ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf### models\n",
    "import numpy as np### math computations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pathlib\n",
    "import io\n",
    "from datetime import datetime\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "# from PIL import Image\n",
    "import albumentations as A\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
    "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
    "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
    "                                     RandomContrast, Rescaling, Resizing, Reshape, LeakyReLU)\n",
    "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
    "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
    "from tensorflow.keras.regularizers import L2, L1\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac1a2850-a5b7-4f4c-a1da-5046a1b2f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(sys.version)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2338ecf-9c70-4e10-afc2-e98a1c27a581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Lenovo'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63e650c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_list=['2007_000027.jpg','2007_000032.jpg','2007_000033.jpg','2007_000039.jpg','2007_000042.jpg','2007_000061.jpg',\n",
    "#           '2007_000063.jpg','2007_000068.jpg','2007_000121.jpg','2007_000123.jpg','2007_000129.jpg','2007_000170.jpg',\n",
    "#           '2007_000175.jpg','2007_000187.jpg','2007_000241.jpg','2007_000243.jpg','2007_000250.jpg','2007_000256.jpg',\n",
    "#           '2007_000272.jpg','2007_000323.jpg','2007_000332.jpg','2007_000333.jpg','2007_000346.jpg','2007_000363.jpg',\n",
    "#           '2007_000364.jpg','2007_000392.jpg','2007_000423.jpg','2007_000452.jpg','2007_000464.jpg','2007_000480.jpg',\n",
    "#           '2007_000491.jpg','2007_000504.jpg','2007_000515.jpg','2007_000528.jpg','2007_000529.jpg','2007_000549.jpg',\n",
    "#           '2007_000559.jpg','2007_000572.jpg','2007_000584.jpg','2007_000629.jpg','2007_000636.jpg','2007_000645.jpg',\n",
    "#           '2007_000648.jpg','2007_000661.jpg','2007_000663.jpg','2007_000664.jpg','2007_000676.jpg','2007_000713.jpg',\n",
    "#           '2007_000720.jpg','2007_000727.jpg','2007_000733.jpg','2007_000738.jpg','2007_000762.jpg','2007_000768.jpg',\n",
    "#           '2007_000783.jpg','2007_000793.jpg','2007_000799.jpg','2007_000804.jpg','2007_000807.jpg','2007_000822.jpg',\n",
    "#           '2007_001299.jpg','2007_001311.jpg','2007_001321.jpg','2007_001340.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dded3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "train_images = '/mnt/c/Users/Lenovo/Desktop/DS/Datasets/VOC2012/JPEGImages/' \n",
    "train_maps = '/mnt/c/Users/Lenovo/Desktop/DS/Datasets/VOC2012/Annotations/' \n",
    "\n",
    "val_images = '/mnt/c/Users/Lenovo/Desktop/DS/Datasets/VOC2012/ValJPEGImages/' \n",
    "val_maps = '/mnt/c/Users/Lenovo/Desktop/DS/Datasets/VOC2012/ValAnnotations/' \n",
    "\n",
    "classes=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable',\n",
    "         'dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
    "\n",
    "B=2\n",
    "N_CLASSES=len(classes)\n",
    "H,W =224,224\n",
    "SPLIT_SIZE=H//32\n",
    "print(SPLIT_SIZE)\n",
    "N_EPOCHS=135\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fc21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in val_list:\n",
    "#     shutil.move(train_maps+name[:-3]+\"xml\", val_maps+name[:-3]+\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in val_list:\n",
    "#     shutil.move(train_images+name, val_images+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb9c31",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9b5570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_xml(filename):\n",
    "  tree = ET.parse(filename)\n",
    "  root = tree.getroot()\n",
    "  size_tree = root.find('size')\n",
    "  height = float(size_tree.find('height').text)\n",
    "  width = float(size_tree.find('width').text)\n",
    "  bounding_boxes=[]\n",
    "  for object_tree in root.findall('object'):\n",
    "    for bounding_box in object_tree.iter('bndbox'):\n",
    "      xmin = (float(bounding_box.find('xmin').text))\n",
    "      ymin = (float(bounding_box.find('ymin').text))\n",
    "      xmax = (float(bounding_box.find('xmax').text))\n",
    "      ymax = (float(bounding_box.find('ymax').text))\n",
    "      break\n",
    "    class_name = object_tree.find('name').text\n",
    "    class_dict={classes[i]:i for i in range(len(classes))}\n",
    "    bounding_box = [\n",
    "        (xmin+xmax)/(2*width),(ymin+ymax)/(2*height),(xmax-xmin)/width,\n",
    "        (ymax-ymin)/height,class_dict[class_name]]\n",
    "    bounding_boxes.append(bounding_box)\n",
    "  return tf.convert_to_tensor(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "984c9d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       "array([[ 0.476     ,  0.736     ,  0.456     ,  0.50133336, 10.        ],\n",
       "       [ 0.409     ,  0.79466665,  0.218     ,  0.41066667,  8.        ],\n",
       "       [ 0.6       ,  0.73066664,  0.192     ,  0.38933334,  8.        ],\n",
       "       [ 0.568     ,  0.564     ,  0.136     ,  0.19466667,  8.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_xml(train_maps+\"2007_000830.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1426162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(bounding_boxes):\n",
    "  output_label=np.zeros((SPLIT_SIZE,SPLIT_SIZE,N_CLASSES+5))\n",
    "  for b in range(len(bounding_boxes)):\n",
    "    grid_x=bounding_boxes[...,b,0]*SPLIT_SIZE\n",
    "    grid_y=bounding_boxes[...,b,1]*SPLIT_SIZE\n",
    "    i=int(grid_x)\n",
    "    j=int(grid_y)\n",
    "\n",
    "    output_label[i,j,0:5]=[1.,grid_x%1,grid_y%1,bounding_boxes[...,b,2],bounding_boxes[...,b,3]]\n",
    "    output_label[i,j,5+int(bounding_boxes[...,b,4])]=1.\n",
    "\n",
    "  return tf.convert_to_tensor(output_label,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d54ab778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25,), dtype=float32, numpy=\n",
       "array([1.        , 0.9760001 , 0.94799995, 0.136     , 0.19466667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_output(preprocess_xml(train_maps+\"2007_000830.xml\"),)[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa3d88e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17061 17061\n",
      "64 64\n"
     ]
    }
   ],
   "source": [
    "im_paths=[]\n",
    "xml_paths=[]\n",
    "\n",
    "val_im_paths=[]\n",
    "val_xml_paths=[]\n",
    "\n",
    "\n",
    "for i in os.listdir(train_maps):\n",
    "    im_paths.append(train_images+i[:-3]+'jpg')\n",
    "    xml_paths.append(train_maps+i)\n",
    "  \n",
    "for i in os.listdir(val_maps):\n",
    "    val_im_paths.append(val_images+i[:-3]+'jpg')\n",
    "    val_xml_paths.append(val_maps+i)\n",
    "  \n",
    "print(len(im_paths),len(xml_paths))\n",
    "print(len(val_im_paths),len(val_xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71dce261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=tf.data.Dataset.from_tensor_slices((im_paths,xml_paths))\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices((val_im_paths,val_xml_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7225ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imbboxes(im_path,xml_path):\n",
    "    img=tf.io.decode_jpeg(tf.io.read_file(im_path))\n",
    "    img=tf.cast(tf.image.resize(img, [H,W]),dtype=tf.float32)\n",
    "\n",
    "    bboxes=tf.numpy_function(func=preprocess_xml, inp=[xml_path], Tout=tf.float32)\n",
    "    return img,bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bdd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.map(get_imbboxes)\n",
    "val_dataset=val_dataset.map(get_imbboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa8c98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3) tf.Tensor([[0.626      0.40727273 0.208      0.34181818 3.        ]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i,j in train_dataset.skip(2):\n",
    "    print(i.shape,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5eb706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, bboxes):\n",
    "    labels = tf.numpy_function(func=generate_output, inp=[bboxes], Tout=(tf.float32))\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48be694-062d-47fe-b4e2-e5a28893b817",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6471711c-9ea4-49d1-8a61-ef58aca82498",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(H,W),\n",
    "    A.RandomCrop(\n",
    "         width=np.random.randint(int(0.9*W),W),\n",
    "         height=np.random.randint(int(0.9*H),H), p=0.5),\n",
    "    A.RandomScale(scale_limit=0.1, interpolation=cv2.INTER_LANCZOS4,p=0.5),\n",
    "    A.HorizontalFlip(p=0.5,),\n",
    "    A.Resize(H,W),\n",
    "    \n",
    "], bbox_params=A.BboxParams(format='yolo', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b275bb7-1121-4a07-a8e2-763e643436db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_albument(image,bboxes):\n",
    "  augmented=transforms(image=image,bboxes=bboxes)\n",
    "  return [tf.convert_to_tensor(augmented[\"image\"],dtype=tf.float32),\n",
    "          tf.convert_to_tensor(augmented[\"bboxes\"],dtype=tf.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cd5e0b0-031a-4bfb-ab58-3e861cb14a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image,bboxes):\n",
    "    aug= tf.numpy_function(func=aug_albument, inp=[image,bboxes], Tout=(tf.float32,tf.float32))\n",
    "    return aug[0],aug[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d209b8-f1cc-47b8-b1c8-52b27c30238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c9554e3-09a1-4bba-9da5-d80c75692f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_augment(img,y):\n",
    "  img = tf.image.random_brightness(img, max_delta=50.)\n",
    "  img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n",
    "  img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n",
    "  img = tf.clip_by_value(img, 0, 255)\n",
    "  labels=tf.numpy_function(func=generate_output, inp=[y], Tout=(tf.float32))\n",
    "  return img,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfd9a926-cea0-45c9-9c2e-55a75288e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.map(preprocess)\n",
    "val_dataset=val_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87529e6e-cd90-4e6b-af51-1bcb0617e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=(\n",
    "  train_dataset.\n",
    "  batch(BATCH_SIZE).\n",
    "  prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_dataset=(\n",
    "  val_dataset.\n",
    "  batch(BATCH_SIZE).\n",
    "  prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944e419-4fa5-4186-a20d-ed9ff0af5a9f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2899b8af-fbae-4a5b-a208-e1dd0f383893",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS=512\n",
    "OUTPUT_DIM=N_CLASSES+5*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4114651f-c0f4-464c-840a-82cb4fdb4470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "# base_model=tf.keras.applications.efficientnet.EfficientNetB1(\n",
    "    weights='imagenet',\n",
    "    input_shape=(H,W,3),\n",
    "    include_top=False,\n",
    ")\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc64521a-e0f4-4b36-b9a0-731ac5f2436f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Undefined shapes are not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 30\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([    \n\u001b[1;32m      2\u001b[0m   base_model,\n\u001b[1;32m      3\u001b[0m   Conv2D(NUM_FILTERS,(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m,kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe_normal\u001b[39m\u001b[38;5;124m'\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m   Reshape((SPLIT_SIZE,SPLIT_SIZE,OUTPUT_DIM)),\n\u001b[1;32m     29\u001b[0m ])\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optree/ops.py:747\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    745\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    746\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Undefined shapes are not supported."
     ]
    }
   ],
   "source": [
    "model=tf.keras.Sequential([    \n",
    "  base_model,\n",
    "  Conv2D(NUM_FILTERS,(3,3), padding = 'same',kernel_initializer='he_normal',),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(alpha=0.1),\n",
    "  \n",
    "  Conv2D(NUM_FILTERS,(3,3),padding = 'same',kernel_initializer='he_normal',),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(alpha=0.1),\n",
    "  \n",
    "  Conv2D(NUM_FILTERS,(3,3),padding = 'same',kernel_initializer='he_normal',),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(alpha=0.1),\n",
    "  \n",
    "  Conv2D(NUM_FILTERS,(3,3),padding = 'same',kernel_initializer='he_normal',),\n",
    "  LeakyReLU(alpha=0.1),\n",
    "\n",
    "  Flatten(),\n",
    "  \n",
    "  Dense(NUM_FILTERS,kernel_initializer='he_normal',),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(alpha=0.1),\n",
    "  \n",
    "  Dropout(0.5),\n",
    "  \n",
    "  Dense(SPLIT_SIZE*SPLIT_SIZE*OUTPUT_DIM,activation='sigmoid'),\n",
    "  \n",
    "  Reshape((SPLIT_SIZE,SPLIT_SIZE,OUTPUT_DIM)),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d51ac2ff-95be-4138-a859-af1ab575c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "  if epoch < 40:\n",
    "    return 1e-3\n",
    "  elif epoch>=40 and epoch<80:\n",
    "    return 5e-4\n",
    "  else:\n",
    "    return 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dd0ffa7-c71f-4bf0-a2c3-7785f4e1bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ace02d4d-008e-41d2-bbc4-0037581cbdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath='C:/Users/Lenovo/Desktop/DS/Datasets/yolo_efficientnet_b1_new.weights.h5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c1a6e-a752-4397-ad4a-27ca8cda370f",
   "metadata": {},
   "source": [
    "### YOLO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26233d2f-0ffd-41e1-ab1b-8366a93ca9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
    "                         boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
    "                         boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
    "                         boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
    "                        axis=-1)\n",
    "\n",
    "    boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
    "                         boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
    "                         boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
    "                         boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
    "                        axis=-1)\n",
    "    lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
    "    rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
    "\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    inter_square = intersection[..., 0] * intersection[..., 1]\n",
    "\n",
    "    square1 = boxes1[..., 2] * boxes1[..., 3]\n",
    "    square2 = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "    union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n",
    "    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cb49b21-31ac-4b32-b1eb-e9327798e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(x,y):\n",
    "  return tf.reduce_sum(tf.square(y-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a5b1fe1-1a23-4b31-abc5-c4be6d3647a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(y_true, y_pred):\n",
    "  target = y_true[...,0]\n",
    "\n",
    "  # Object Loss\n",
    "  y_pred_extract = tf.gather_nd(y_pred, tf.where(target[:]==1))\n",
    "  y_target_extract = tf.gather_nd(y_true, tf.where(target[:]==1))\n",
    "  \n",
    "  rescaler = tf.where(target[:]==1)*SPLIT_SIZE\n",
    "  upscaler_1 = tf.concat([rescaler[:,1:],tf.zeros([len(rescaler),2], dtype=tf.int64)],axis=-1)\n",
    "  \n",
    "  target_upscaler_2 = tf.repeat([[float(SPLIT_SIZE),float(SPLIT_SIZE),H,W]],\n",
    "                       repeats=[len(rescaler)], axis=0)*tf.cast(y_target_extract[...,1:5], dtype = tf.float32)\n",
    "  pred_1_upscaler_2 = tf.repeat([[float(SPLIT_SIZE),float(SPLIT_SIZE),H,W]],\n",
    "                      repeats=[len(rescaler)], axis=0)*tf.cast(y_pred_extract[...,1:5], dtype = tf.float32)\n",
    "  pred_2_upscaler_2 = tf.repeat([[float(SPLIT_SIZE),float(SPLIT_SIZE),H,W]],\n",
    "                      repeats=[len(rescaler)], axis=0)*tf.cast(y_pred_extract[...,6:10], dtype = tf.float32)\n",
    "  \n",
    "  target_orig = tf.cast(upscaler_1, dtype = tf.float32)+target_upscaler_2\n",
    "  pred_1_orig = tf.cast(upscaler_1, dtype = tf.float32)+pred_1_upscaler_2\n",
    "  pred_2_orig = tf.cast(upscaler_1, dtype = tf.float32)+pred_2_upscaler_2\n",
    "  \n",
    "  mask =tf.cast(tf.math.greater(compute_iou(target_orig,pred_2_orig),\n",
    "                                         compute_iou(target_orig,pred_1_orig)),dtype=tf.int32)\n",
    "  \n",
    "  y_pred_joined=tf.transpose(tf.concat([tf.expand_dims(y_pred_extract[...,0],axis=0),\n",
    "                        tf.expand_dims(y_pred_extract[...,5],axis=0)],axis=0))\n",
    "  \n",
    "  obj_pred = tf.gather_nd(y_pred_joined,tf.stack([tf.range(len(rescaler)),mask],axis=-1))\n",
    "  \n",
    "  object_loss = difference(tf.cast(obj_pred,dtype =tf.float32)\n",
    "                            ,tf.cast(tf.ones([len(rescaler)]),dtype=tf.float32))\n",
    "\n",
    "  # For No object\n",
    "  y_pred_extract = tf.gather_nd(y_pred[...,0:B*5], tf.where(target[:]==0))\n",
    "  y_target_extract = tf.zeros(len(y_pred_extract))\n",
    "\n",
    "  no_object_loss_1 = difference(tf.cast(y_pred_extract[...,0],dtype =tf.float32)\n",
    "                            ,tf.cast(y_target_extract,dtype=tf.float32))\n",
    "  \n",
    "  no_object_loss_2 = difference(tf.cast(y_pred_extract[...,5],dtype =tf.float32)\n",
    "                            ,tf.cast(y_target_extract,dtype=tf.float32))\n",
    "  \n",
    "  no_object_loss = no_object_loss_1+no_object_loss_2\n",
    "\n",
    "  # For Object class loss\n",
    "  y_pred_extract = tf.gather_nd(y_pred[...,10:],tf.where(target[:]==1))\n",
    "  class_extract = tf.gather_nd(y_true[...,5:],tf.where(target[:]==1))\n",
    "\n",
    "  class_loss = difference(tf.cast(y_pred_extract,dtype =tf.float32)\n",
    "                                ,tf.cast(class_extract,dtype=tf.float32))\n",
    "\n",
    "  # For object bounding box loss\n",
    "  y_pred_extract = tf.gather_nd(y_pred[...,0:B*5], tf.where(target[:]==1))\n",
    "  centre_joined=tf.stack([y_pred_extract[...,1:3],y_pred_extract[...,6:8]],axis=1)\n",
    "  centre_pred = tf.gather_nd(centre_joined,tf.stack([tf.range(len(rescaler)),mask],axis=-1))\n",
    "  centre_target = tf.gather_nd(y_true[...,1:3], tf.where(target[:]==1))\n",
    "  \n",
    "  centre_loss = difference(centre_pred,centre_target)\n",
    "    \n",
    "  size_joined=tf.stack([y_pred_extract[...,3:5],y_pred_extract[...,8:10]],axis=1)\n",
    "\n",
    "  size_pred = tf.gather_nd(size_joined,tf.stack([tf.range(len(rescaler)),mask],axis=-1))\n",
    "  size_target = tf.gather_nd(y_true[...,3:5], tf.where(target[:]==1))\n",
    "  \n",
    "  size_loss = difference(tf.math.sqrt(tf.math.abs(size_pred)),tf.math.sqrt(tf.math.abs(size_target)))\n",
    "  box_loss = centre_loss+size_loss\n",
    "  \n",
    "  lambda_coord = 5.0\n",
    "  lambda_no_obj = 0.5\n",
    "\n",
    "  loss = object_loss + (lambda_no_obj*no_object_loss)+ tf.cast(lambda_coord*box_loss,dtype=tf.float32)+ tf.cast(class_loss,dtype=tf.float32) \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa3bbe39-1c29-4b4a-8be9-f2aedd7e5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss=yolo_loss,\n",
    "  optimizer=Adam(1e-3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1d28bc4-756e-48d9-aa13-4f9fd3f46ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=val_dataset,\n",
    "  verbose=1,\n",
    "  epochs=50,\n",
    "  callbacks = [lr_callback,callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a618f-da7f-4ce3-b535-280d74131f5d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ba12e-b824-4497-aadb-a015906299e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = \"/mnt/c/Users/Lenovo/Desktop/DS/Datasets/Outputs\"\n",
    "COCO_PATH='/mnt/c/Users/Lenovo/Desktop/DS/Datasets/coco128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2926df6-5611-4188-86f3-21d8210e2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(filename):\n",
    "  try:\n",
    "    test_path=COCO_PATH+filename\n",
    "\n",
    "    print(test_path)\n",
    "    \n",
    "    img=cv2.resize(cv2.imread(test_path),(H,W))\n",
    "\n",
    "    image=tf.io.decode_jpeg(tf.io.read_file(test_path))\n",
    "    image=tf.image.resize(image, [H,W])\n",
    "\n",
    "    output=model.predict(np.expand_dims(image, axis = 0))\n",
    "\n",
    "    THRESH=.25\n",
    "\n",
    "    object_positions=tf.concat(\n",
    "        [tf.where(output[...,0]>=THRESH),tf.where(output[...,5]>=THRESH)],axis=0)\n",
    "    print(object_positions)\n",
    "    selected_output=tf.gather_nd(output,object_positions)\n",
    "    print(selected_output)\n",
    "    final_boxes=[]\n",
    "    final_scores=[]\n",
    "\n",
    "    for i,pos in enumerate(object_positions):\n",
    "      for j in range(2):      \n",
    "        if selected_output[i][j*5]>THRESH:\n",
    "          output_box=tf.cast(output[pos[0]][pos[1]][pos[2]][(j*5)+1:(j*5)+5],dtype=tf.float32)\n",
    "          \n",
    "          x_centre=(tf.cast(pos[1],dtype=tf.float32)+output_box[0])*32\n",
    "          y_centre=(tf.cast(pos[2],dtype=tf.float32)+output_box[1])*32\n",
    "\n",
    "          x_width,y_height=tf.math.abs(H*output_box[2]),tf.math.abs(W*output_box[3])\n",
    "          \n",
    "          x_min,y_min=int(x_centre-(x_width/2)),int(y_centre-(y_height/2))\n",
    "          x_max,y_max=int(x_centre+(x_width/2)),int(y_centre+(y_height/2))\n",
    "\n",
    "          if(x_min<=0):x_min=0\n",
    "          if(y_min<=0):y_min=0\n",
    "          if(x_max>=W):x_max=W\n",
    "          if(y_max>=H):y_max=H\n",
    "          final_boxes.append(\n",
    "              [x_min,y_min,x_max,y_max,\n",
    "              str(classes[tf.argmax(selected_output[...,10:],axis=-1)[i]])])\n",
    "          final_scores.append(selected_output[i][j*5])\n",
    "    print(final_scores)\n",
    "    print('finalboxes',final_boxes)\n",
    "    final_boxes=np.array(final_boxes)\n",
    "    \n",
    "    object_classes=final_boxes[...,4]\n",
    "    nms_boxes=final_boxes[...,0:4]\n",
    "\n",
    "    nms_output=tf.image.non_max_suppression(\n",
    "        nms_boxes,final_scores,max_output_size=100,iou_threshold=0.2,\n",
    "        score_threshold=float('-inf')\n",
    "    )\n",
    "    print(nms_output)\n",
    "\n",
    "    for i in nms_output:\n",
    "      cv2.rectangle(\n",
    "          img,\n",
    "          (int(final_boxes[i][0]),int(final_boxes[i][1])),\n",
    "          (int(final_boxes[i][2]),int(final_boxes[i][3])),(0,0,255),1)\n",
    "      cv2.putText(\n",
    "          img,\n",
    "          final_boxes[i][-1],\n",
    "          (int(final_boxes[i][0]),int(final_boxes[i][1])+15),\n",
    "          cv2.FONT_HERSHEY_COMPLEX_SMALL,1,(2,225,155),1\n",
    "          )\n",
    "        \n",
    "    cv2.imwrite('/content/outputs/'+filename[:-4]+'_det'+'.jpg',cv2.resize(img,(384,384)))\n",
    "  except:\n",
    "    print(\"NO object found !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67badf7-6fba-4ca4-af64-5d33bea5a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(COCO_PATH):\n",
    "  model_test(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
